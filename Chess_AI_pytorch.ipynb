{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca913854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57f3c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_chess\n",
    "import chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec46463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_generator(length):\n",
    "    lst = []\n",
    "    for i in range(length):\n",
    "        lst.append(0)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d304b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#white pieces: rook (r) = 1, knight (n) = 2, bishop (b) = 3, queen (q) = 4, king (k) = 5, pawn (p) = 6\n",
    "#black pieces: rook (R) = 11, knight (N) = 12, bishop (B) = 13, queen (Q) = 14, king (K) = 15, pawn (P) = 16\n",
    "#switching ranks: / = 9\n",
    "#empty squares: number = same number of zeros. Ex: 6 = 0, 0, 0, 0, 0, 0\n",
    "#results are too long\n",
    "#try having a list (int_board) with 8 elements, one for each rank\n",
    "#split fen at '/'\n",
    "def board_to_int(board):\n",
    "    if isinstance(board, list):\n",
    "        real_board = board[0]\n",
    "        if isinstance(real_board, int):\n",
    "            return board\n",
    "        else:\n",
    "            board = real_board\n",
    "    fen = board.board_fen()\n",
    "    fen_by_rank = fen.split('/')\n",
    "    int_board = []\n",
    "    for rank in fen_by_rank:\n",
    "        int_rank = ''\n",
    "        for symbol in rank:\n",
    "            #white pieces\n",
    "            if symbol == 'r':\n",
    "                int_rank +='1'\n",
    "            elif symbol == 'n':\n",
    "                int_rank += '2'\n",
    "            elif symbol == 'b':\n",
    "                int_rank += '3'\n",
    "            elif symbol == 'q':\n",
    "                int_rank += '4'\n",
    "            elif symbol == 'k':\n",
    "                int_rank += '5'\n",
    "            elif symbol == 'p':\n",
    "                int_rank += '6'\n",
    "        \n",
    "            #black pieces\n",
    "            elif symbol == 'R':\n",
    "                int_rank += '11'\n",
    "            elif symbol == 'N':\n",
    "                int_rank += '12'\n",
    "            elif symbol == 'B':\n",
    "                int_rank += '13'\n",
    "            elif symbol == 'Q':\n",
    "                int_rank += '14'\n",
    "            elif symbol == 'K':\n",
    "                int_rank += '15'\n",
    "            elif symbol == 'P':\n",
    "                int_rank += '16'\n",
    "            \n",
    "            #empty squares and lines between ranks\n",
    "            else:\n",
    "                for i in range(int(symbol)):\n",
    "                    int_rank += '0'\n",
    "        int_board.append(int(int_rank))\n",
    "    return int_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5041cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 1, b = 2, c = 3, d = 4, e = 5, f = 6, g = 7, h = 8 (n for knight = 0)\n",
    "def move_to_int(move):\n",
    "    if isinstance(move, int):\n",
    "        return move\n",
    "    if isinstance(move, list):\n",
    "        move = move[0]\n",
    "        if isinstance(move, int):\n",
    "            return move\n",
    "    uci = move.uci()\n",
    "    int_move = ''\n",
    "    for symbol in uci:\n",
    "        if symbol == 'a':\n",
    "            int_move += '1'\n",
    "        elif symbol == 'b':\n",
    "            int_move += '2'\n",
    "        elif symbol == 'c':\n",
    "            int_move += '3'\n",
    "        elif symbol == 'd':\n",
    "            int_move += '4'\n",
    "        elif symbol == 'e':\n",
    "            int_move += '5'\n",
    "        elif symbol == 'f':\n",
    "            int_move += '6'\n",
    "        elif symbol == 'g':\n",
    "            int_move += '7'\n",
    "        elif symbol == 'h':\n",
    "            int_move += '8'\n",
    "        elif symbol == 'r' or symbol == 'R':\n",
    "            int_move += '10'\n",
    "        elif symbol == 'n' or symbol == 'N':\n",
    "            int_move += '11'\n",
    "        elif symbol == 'b' or symbol == 'B':\n",
    "            int_move += '12'\n",
    "        elif symbol == 'q' or symbol == 'Q':\n",
    "            int_move += '13'\n",
    "        else:\n",
    "            int_move += symbol\n",
    "    int_move_final = int(int_move)\n",
    "    return int_move_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5999ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_move_white(number):\n",
    "    if isinstance(number, int):\n",
    "        uci = ''\n",
    "        num_str = str(number)\n",
    "        \n",
    "        first = num_str[0] #will be letter\n",
    "        if first == '1':\n",
    "            uci += 'a'\n",
    "        elif first == '2':\n",
    "            uci += 'b'\n",
    "        elif first == '3':\n",
    "            uci += 'c'\n",
    "        elif first == '4':\n",
    "            uci += 'd'\n",
    "        elif first == '5':\n",
    "            uci += 'e'\n",
    "        elif first == '6':\n",
    "            uci += 'f'\n",
    "        elif first == '7':\n",
    "            uci += 'g'\n",
    "        elif first == '8':\n",
    "            uci += 'h'\n",
    "        \n",
    "        uci += num_str[1]\n",
    "        \n",
    "        third = num_str[2] #will be letter\n",
    "        if first == '1':\n",
    "            uci += 'a'\n",
    "        elif third == '2':\n",
    "            uci += 'b'\n",
    "        elif third == '3':\n",
    "            uci += 'c'\n",
    "        elif third == '4':\n",
    "            uci += 'd'\n",
    "        elif third == '5':\n",
    "            uci += 'e'\n",
    "        elif third == '6':\n",
    "            uci += 'f'\n",
    "        elif third == '7':\n",
    "            uci += 'g'\n",
    "        elif third == '8':\n",
    "            uci += 'h'\n",
    "        \n",
    "        uci += num_str[3]\n",
    "        \n",
    "        if len(num_str) == 6:\n",
    "            last = num_str[4:]\n",
    "            if last == '10':\n",
    "                uci += 'R'\n",
    "            elif last == '11':\n",
    "                uci += 'N'\n",
    "            elif last == '12':\n",
    "                uci += 'B'\n",
    "            elif last == '13':\n",
    "                uci += 'Q'\n",
    "        move = chess.Move.from_uci(uci)\n",
    "        return move\n",
    "    else:\n",
    "        return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d76063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_move_black(number):\n",
    "    if isinstance(number, int):\n",
    "        uci = ''\n",
    "        num_str = str(number)\n",
    "        \n",
    "        first = num_str[0] #will be letter\n",
    "        if first == '1':\n",
    "            uci += 'a'\n",
    "        elif first == '2':\n",
    "            uci += 'b'\n",
    "        elif first == '3':\n",
    "            uci += 'c'\n",
    "        elif first == '4':\n",
    "            uci += 'd'\n",
    "        elif first == '5':\n",
    "            uci += 'e'\n",
    "        elif first == '6':\n",
    "            uci += 'f'\n",
    "        elif first == '7':\n",
    "            uci += 'g'\n",
    "        elif first == '8':\n",
    "            uci += 'h'\n",
    "        \n",
    "        uci += num_str[1]\n",
    "        \n",
    "        third = num_str[2] #will be letter\n",
    "        if first == '1':\n",
    "            uci += 'a'\n",
    "        elif third == '2':\n",
    "            uci += 'b'\n",
    "        elif third == '3':\n",
    "            uci += 'c'\n",
    "        elif third == '4':\n",
    "            uci += 'd'\n",
    "        elif third == '5':\n",
    "            uci += 'e'\n",
    "        elif third == '6':\n",
    "            uci += 'f'\n",
    "        elif third == '7':\n",
    "            uci += 'g'\n",
    "        elif third == '8':\n",
    "            uci += 'h'\n",
    "        \n",
    "        uci += num_str[3]\n",
    "        \n",
    "        if len(num_str) == 6:\n",
    "            last = num_str[4:]\n",
    "            if last == '10':\n",
    "                uci += 'r'\n",
    "            elif last == '11':\n",
    "                uci += 'n'\n",
    "            elif last == '12':\n",
    "                uci += 'b'\n",
    "            elif last == '13':\n",
    "                uci += 'q'\n",
    "        move = chess.Move.from_uci(uci)\n",
    "        return move\n",
    "    else:\n",
    "        return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ddcd4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, memory_size):\n",
    "        self.memory_size = memory_size\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "        self.state_memory = list_generator(self.memory_size)\n",
    "        self.new_state_memory = list_generator(self.memory_size)\n",
    "        self.action_memory = list_generator(self.memory_size)\n",
    "        self.reward_memory = list_generator(self.memory_size)\n",
    "        self.terminal_memory = list_generator(self.memory_size)\n",
    "    \n",
    "    def store_game(self, state, action, reward, new_state, done):\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = new_state\n",
    "        self.terminal_memory[index] = done\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_memory = min(self.memory_counter, self.memory_size)\n",
    "        batch = np.random.choice(max_memory, batch_size, replace=False)\n",
    "        \n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        new_states = []\n",
    "        terminal = []\n",
    "        \n",
    "        for i in batch:\n",
    "            states.append(self.state_memory[i])\n",
    "            actions.append(self.action_memory[i])\n",
    "            rewards.append(self.reward_memory[i])\n",
    "            new_states.append(self.new_state_memory[i])\n",
    "            terminal.append(self.terminal_memory[i])\n",
    "\n",
    "        return states, actions, rewards, new_states, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51d97c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, lr, n_actions, name, input_dims, checkpoint_directory):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(input_dims, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.value_layer = nn.Linear(128, 1)\n",
    "        self.advantage_layer = nn.Linear(128, n_actions)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_directory, name+'_dqn.zip')\n",
    "\n",
    "    def forward(self, state):\n",
    "        layer_1_output = F.relu(self.layer1(state))\n",
    "        layer_2_output = F.relu(self.layer2(layer_1_output))\n",
    "        value = self.value_layer(layer_2_output)\n",
    "        advantage = self.advantage_layer(layer_2_output)\n",
    "\n",
    "        return value, advantage\n",
    "  \n",
    "    def save_checkpoint(self):\n",
    "        print('saving checkpoint')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "  \n",
    "    def load_checkpoint(self):\n",
    "        print('loading checkpoint')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d79da75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, discount_rate, epsilon, lr, n_actions, input_dims, memory_size, batch_size, \n",
    "                 checkpoint_directory, epsilon_min=0.01, epsilon_dec=5e-7, replace=1000):\n",
    "        self.discount_rate = discount_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.lr = lr\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_dec = epsilon_dec\n",
    "        self.batch_size = batch_size\n",
    "        self.learn_step_counter = 0\n",
    "        self.replace = replace\n",
    "\n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.memory = ReplayBuffer(memory_size)\n",
    "\n",
    "        self.eval_network = Network(lr, n_actions, 'eval_network', input_dims, checkpoint_directory)\n",
    "        self.next_network = Network(lr, n_actions, 'next_network', input_dims, checkpoint_directory)\n",
    "\n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        game_states = []\n",
    "        game_actions = []\n",
    "        game_rewards = []\n",
    "        game_new_states = []\n",
    "        game_dones = []\n",
    "\n",
    "        game_states.append(state)\n",
    "        game_actions.append(action)\n",
    "        game_rewards.append(reward)\n",
    "        game_new_states.append(new_state)\n",
    "        game_dones.append(done)\n",
    "        if done == 1:\n",
    "            self.memory.store_game(game_states, game_actions, game_rewards, game_new_states, done)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.FloatTensor([board_to_int(observation)]).to(self.eval_network.device)\n",
    "            _, advantage = self.eval_network.forward(state)\n",
    "            action = T.argmax(advantage).item()\n",
    "            if observation.turn:\n",
    "                try:\n",
    "                    action = int_to_move_white(action)\n",
    "                except:\n",
    "                    return action\n",
    "            else:\n",
    "                try:\n",
    "                    action = int_to_move_black(action)\n",
    "                except:\n",
    "                    return action\n",
    "        else:\n",
    "            action = np.random.choice(env.legal_moves)\n",
    "        \n",
    "        return action\n",
    "  \n",
    "    def decrement_epsilon(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            if (self.epsilon - self.epsilon_dec) <= self.epsilon_min:\n",
    "                self.epsilon = self.epsilon_min\n",
    "            else:\n",
    "                self.epsilon = self.epsilon - self.epsilon_dec\n",
    "                \n",
    "    def replace_target_network(self):\n",
    "        if  self.replace is not None and self.learn_step_counter % self.replace == 0:\n",
    "            self.next_network.load_state_dict(self.eval_network.state_dict())\n",
    "\n",
    "\n",
    "    def save_models(self):\n",
    "        self.eval_network.save_checkpoint()\n",
    "        self.next_network.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.eval_network.load_checkpoint()\n",
    "        self.next_network.load_checkpoint()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.memory_counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.eval_network.optimizer.zero_grad()\n",
    "    \n",
    "        self.replace_target_network()\n",
    "\n",
    "        state, action, reward, new_state, done = self.memory.sample_buffer(self.batch_size)\n",
    "        \n",
    "        formatted_state = []\n",
    "        for thing in state:\n",
    "            formatted_thing = board_to_int(thing)\n",
    "            formatted_state.append(formatted_thing)\n",
    "        states = T.FloatTensor(formatted_state).to(self.eval_network.device)\n",
    "        \n",
    "        formatted_action = []\n",
    "        for thing in action:\n",
    "            formatted_thing = move_to_int(thing)\n",
    "            formatted_action.append(formatted_thing)\n",
    "        actions = T.tensor(formatted_action).to(self.eval_network.device)\n",
    "        \n",
    "        formatted_reward = []\n",
    "        for thing in reward:\n",
    "            formatted_thing = thing[0]\n",
    "            formatted_reward.append(formatted_thing)\n",
    "        rewards = T.tensor(formatted_reward).to(self.eval_network.device)\n",
    "        \n",
    "        formatted_new_state = []\n",
    "        for thing in new_state:\n",
    "            formatted_thing = board_to_int(thing)\n",
    "            formatted_new_state.append(formatted_thing)\n",
    "        new_states = T.FloatTensor(formatted_new_state).to(self.eval_network.device)\n",
    "        \n",
    "        dones = T.tensor(done).to(self.eval_network.device)\n",
    "        \n",
    "        indices = np.arange(self.batch_size)\n",
    "        \n",
    "        value_states = []\n",
    "        advantage_states = []\n",
    "        for observation in states:\n",
    "            value_observation, advantage_observation = self.eval_network.forward(T.stack([observation]))\n",
    "            value_states.append(value_observation)\n",
    "            advantage_states.append(advantage_observation)\n",
    "            \n",
    "        advantage_states_max = []\n",
    "        for item in advantage_states:\n",
    "            max_item = item.max()\n",
    "            advantage_states_max.append(max_item)\n",
    "        \n",
    "        value_states = T.tensor(value_states)\n",
    "        advantage_states = T.tensor(advantage_states_max)\n",
    "        \n",
    "        value_new_states = []\n",
    "        advantage_new_states = []\n",
    "        for observation in new_states:\n",
    "            value_new_observation, advantage_new_observation = self.next_network.forward(T.stack([observation]))\n",
    "            value_new_states.append(value_new_observation)\n",
    "            advantage_new_states.append(advantage_new_observation)\n",
    "        \n",
    "        advantage_new_states_max = []\n",
    "        for item in advantage_new_states:\n",
    "            max_item = item.max()\n",
    "            advantage_new_states_max.append(max_item)\n",
    "        \n",
    "        value_new_states = T.tensor(value_new_states)\n",
    "        advantage_new_states = T.tensor(advantage_new_states_max)\n",
    "\n",
    "        value_new_states_eval = []\n",
    "        advantage_new_states_eval = []\n",
    "        for observation in new_states:\n",
    "            value_new_observation_eval, advantage_new_observation_eval = self.eval_network.forward(T.stack([observation]))\n",
    "            value_new_states_eval.append(value_new_observation_eval)\n",
    "            advantage_new_states_eval.append(advantage_new_observation_eval)\n",
    "        \n",
    "        advantage_new_states_eval_max = []\n",
    "        for item in advantage_new_states_eval:\n",
    "            max_item = item.max()\n",
    "            advantage_new_states_eval_max.append(max_item)\n",
    "        \n",
    "        value_new_states_eval = T.tensor(value_new_states_eval)\n",
    "        advantage_new_states_eval = T.tensor(advantage_new_states_eval_max)\n",
    "\n",
    "        q_pred = T.add(value_states, (advantage_states - advantage_states.mean(dim=0, keepdim=True)))[indices]\n",
    "        q_next = T.add(value_new_states, (advantage_new_states - advantage_new_states.mean(dim=0, keepdim=True)))\n",
    "        \n",
    "        q_eval = T.add(value_new_states_eval, (advantage_new_states_eval - advantage_new_states_eval.mean(dim=0, keepdim=True)))\n",
    "        \n",
    "        max_actions = T.argmax(q_eval, dim=0)\n",
    "\n",
    "        q_target = rewards + self.discount_rate*q_next\n",
    "        q_target[dones] = 0.0\n",
    "        \n",
    "        loss = self.eval_network.loss(q_target, q_pred)\n",
    "        loss.requires_grad_()\n",
    "        loss.backward()\n",
    "        self.eval_network.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c6d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Chess-v0')\n",
    "num_games = 20\n",
    "load_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdea94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(discount_rate=0.95, epsilon=1.0, lr=5e-4, n_actions=64**2, input_dims=8, memory_size=1000000,\n",
    "              batch_size=64, checkpoint_directory=r'C:\\Users\\cghat\\Documents\\Reinforcement Learning Project\\ChessFolder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67ad8925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint\n",
      "loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "if load_checkpoint:\n",
    "    agent.load_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2bbc56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "saving checkpoint\n",
      "saving checkpoint\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_games):\n",
    "    print(i)\n",
    "    done = 0\n",
    "    observation = board_to_int(env.reset())\n",
    "    \n",
    "    while done != 1:\n",
    "        if observation == [12345321, 66666666, 0, 0, 0, 0, 1616161616161616, 1112131415131211]:\n",
    "            observation = chess.Board()\n",
    "        action = agent.choose_action(observation)\n",
    "        if isinstance(action, int):\n",
    "            new_observation = observation\n",
    "            reward = -1000000\n",
    "            done = 1\n",
    "            agent.store_transition(observation, action, reward, new_observation, 1)\n",
    "            agent.learn()\n",
    "        else:\n",
    "            try:\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                agent.store_transition(observation, action, reward, new_observation, int(done))\n",
    "                agent.learn()\n",
    "                observation = new_observation\n",
    "            except ValueError:\n",
    "                new_observation = observation\n",
    "                reward = -1000000\n",
    "                done = 1\n",
    "                agent.store_transition(observation, action, reward, new_observation, 1)\n",
    "                agent.learn()\n",
    "    \n",
    "    if i > 0 and i % 10 == 0:\n",
    "        agent.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3dce792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI is white\n",
    "def play_AI_white():\n",
    "    agent.load_models()\n",
    "    observation = board_to_int(env.reset())\n",
    "    done = 0\n",
    "    \n",
    "    while done != 1:\n",
    "        if observation == [12345321, 66666666, 0, 0, 0, 0, 1616161616161616, 1112131415131211]:\n",
    "            observation = chess.Board()\n",
    "        if observation.turn:\n",
    "            action = agent.choose_action(observation)\n",
    "            print(action)\n",
    "            try:\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                agent.store_transition(observation, action, reward, new_observation, int(done))\n",
    "                observation = new_observation\n",
    "            except ValueError:\n",
    "                raise ValueError('Agent made an invalid move')\n",
    "        else:\n",
    "            action = chess.Move.from_uci(input('Black\\'s move: '))\n",
    "            try:\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                agent.store_transition(observation, action, reward, new_observation, int(done))\n",
    "                observation = new_observation\n",
    "            except ValueError:\n",
    "                raise ValueError('User made an invalid move')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05e24ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI is black\n",
    "def play_AI_black():\n",
    "    agent.load_models()\n",
    "    observation = board_to_int(env.reset())\n",
    "    done = 0\n",
    "    \n",
    "    while done != 1:\n",
    "        if observation == [12345321, 66666666, 0, 0, 0, 0, 1616161616161616, 1112131415131211]:\n",
    "            observation = chess.Board()\n",
    "        if observation.turn:\n",
    "            action = chess.Move.from_uci(input('White\\'s move: '))\n",
    "            try:\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                agent.store_transition(observation, action, reward, new_observation, int(done))\n",
    "                observation = new_observation\n",
    "            except ValueError:\n",
    "                raise ValueError('User made an invalid move')\n",
    "        else:\n",
    "            action = agent.choose_action(observation)\n",
    "            print(action)\n",
    "            try:\n",
    "                new_observation, reward, done, info = env.step(action)\n",
    "                agent.store_transition(observation, action, reward, new_observation, int(done))\n",
    "                observation = new_observation\n",
    "            except ValueError:\n",
    "                raise ValueError('Agent made an invalid move')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "593d8443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading checkpoint\n",
      "loading checkpoint\n",
      "White's move: c2c4\n",
      "e7e6\n",
      "White's move: g2g3\n",
      "h7h6\n",
      "White's move: f1g2\n",
      "g8f6\n",
      "White's move: d2d3\n",
      "f6d5\n",
      "White's move: c4d5\n",
      "b8a6\n",
      "White's move: d5e6\n",
      "d7d6\n",
      "White's move: e6f7\n",
      "e8d7\n",
      "White's move: g1f3\n",
      "g7g6\n",
      "White's move: g2h3\n",
      "d7c6\n",
      "White's move: d1c2\n",
      "a6c5\n",
      "White's move: f3d4\n",
      "c6d5\n",
      "White's move: c2c4\n",
      "d5e5\n",
      "White's move: c1f4\n",
      "e5f6\n",
      "White's move: h3c8\n",
      "b7b5\n",
      "White's move: c4b5\n",
      "c5d3\n",
      "White's move: e2d3\n",
      "g6g5\n",
      "White's move: b5f5\n",
      "f6g7\n",
      "White's move: d4e6\n"
     ]
    }
   ],
   "source": [
    "#playing with human sequence (switch function depending on which color you want AI to be)\n",
    "play_AI_black()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChessReinforcementLearning",
   "language": "python",
   "name": "chessreinforcementlearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
